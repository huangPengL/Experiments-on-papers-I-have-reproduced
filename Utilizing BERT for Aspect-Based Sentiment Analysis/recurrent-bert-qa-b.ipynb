{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-19T11:14:36.095329Z","iopub.execute_input":"2022-09-19T11:14:36.096036Z","iopub.status.idle":"2022-09-19T11:14:36.111985Z","shell.execute_reply.started":"2022-09-19T11:14:36.095950Z","shell.execute_reply":"2022-09-19T11:14:36.110857Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/bert-qa-b-for-absa/dev_QA_B.tsv\n/kaggle/input/bert-qa-b-for-absa/train_QA_B.tsv\n/kaggle/input/bert-qa-b-for-absa/test_QA_B.tsv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:36.118481Z","iopub.execute_input":"2022-09-19T11:14:36.119570Z","iopub.status.idle":"2022-09-19T11:14:45.536256Z","shell.execute_reply.started":"2022-09-19T11:14:36.119531Z","shell.execute_reply":"2022-09-19T11:14:45.535097Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\n# identify and specify the GPU as the device, later in training loop we will load data into device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 19\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)\n\n    \nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:45.538448Z","iopub.execute_input":"2022-09-19T11:14:45.538855Z","iopub.status.idle":"2022-09-19T11:14:46.113186Z","shell.execute_reply.started":"2022-09-19T11:14:45.538810Z","shell.execute_reply":"2022-09-19T11:14:46.112160Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# loading and preprocessing data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv(\"/kaggle/input/bert-qa-b-for-absa/train_QA_B.tsv\",skiprows=1, delimiter='\\t', header = None, names=['id','sentence1','sentence2','label'])\ndf_test = pd.read_csv(\"/kaggle/input/bert-qa-b-for-absa/test_QA_B.tsv\",skiprows=1, delimiter='\\t', header = None,names=['id','sentence1','sentence2','label'])\ndf_val = pd.read_csv(\"/kaggle/input/bert-qa-b-for-absa/dev_QA_B.tsv\",skiprows=1, delimiter='\\t', header = None,names=['id','sentence1','sentence2','label'])\n\nprint(f\"train data len: {len(df_train)}\")\nprint(f\"test data len: {len(df_test)}\")\nprint(f\"val data len: {len(df_val)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:46.116389Z","iopub.execute_input":"2022-09-19T11:14:46.117530Z","iopub.status.idle":"2022-09-19T11:14:46.235086Z","shell.execute_reply.started":"2022-09-19T11:14:46.117490Z","shell.execute_reply":"2022-09-19T11:14:46.231143Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"train data len: 45024\ntest data len: 22548\nval data len: 11244\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df_train[:10])\n\nprint(\"\\n\")\nprint(df_train['label'].unique())\n\nunique_labels = df_train['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:46.236720Z","iopub.execute_input":"2022-09-19T11:14:46.237125Z","iopub.status.idle":"2022-09-19T11:14:46.249495Z","shell.execute_reply.started":"2022-09-19T11:14:46.237085Z","shell.execute_reply":"2022-09-19T11:14:46.248345Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"   id                    sentence1  \\\n0   0  location - 2 , location - 1   \n1   0  location - 2 , location - 1   \n2   0  location - 2 , location - 1   \n3   0  location - 2 , location - 1   \n4   0  location - 2 , location - 1   \n5   0  location - 2 , location - 1   \n6   0  location - 2 , location - 1   \n7   0  location - 2 , location - 1   \n8   0  location - 2 , location - 1   \n9   0  location - 2 , location - 1   \n\n                                           sentence2  label  \n0  the polarity of the aspect general of location...      1  \n1  the polarity of the aspect general of location...      0  \n2  the polarity of the aspect general of location...      0  \n3  the polarity of the aspect price of location -...      1  \n4  the polarity of the aspect price of location -...      0  \n5  the polarity of the aspect price of location -...      0  \n6  the polarity of the aspect safety of location ...      1  \n7  the polarity of the aspect safety of location ...      0  \n8  the polarity of the aspect safety of location ...      0  \n9  the polarity of the aspect transit location of...      1  \n\n\n[1 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# 将字符串映射成整型\ndef train_str_2_int(df):\n    labelEncoder = LabelEncoder()\n    df['label_enc'] = labelEncoder.fit_transform(df['label'])\n    \n    # 原地给列改名\n    df.rename(columns = {'label': 'label_desc'}, inplace = True)\n    df.rename(columns = {'label_enc': 'label'}, inplace = True)\n\n    \ntrain_str_2_int(df_train)\ntrain_str_2_int(df_test)\ntrain_str_2_int(df_val)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:46.251365Z","iopub.execute_input":"2022-09-19T11:14:46.251738Z","iopub.status.idle":"2022-09-19T11:14:46.590438Z","shell.execute_reply.started":"2022-09-19T11:14:46.251702Z","shell.execute_reply":"2022-09-19T11:14:46.589514Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"print(df_train[0:10])\nprint(df_test[0:10])\nprint(df_val[0:10])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:46.591858Z","iopub.execute_input":"2022-09-19T11:14:46.592195Z","iopub.status.idle":"2022-09-19T11:14:46.604303Z","shell.execute_reply.started":"2022-09-19T11:14:46.592159Z","shell.execute_reply":"2022-09-19T11:14:46.603111Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"   id                    sentence1  \\\n0   0  location - 2 , location - 1   \n1   0  location - 2 , location - 1   \n2   0  location - 2 , location - 1   \n3   0  location - 2 , location - 1   \n4   0  location - 2 , location - 1   \n5   0  location - 2 , location - 1   \n6   0  location - 2 , location - 1   \n7   0  location - 2 , location - 1   \n8   0  location - 2 , location - 1   \n9   0  location - 2 , location - 1   \n\n                                           sentence2  label_desc  label  \n0  the polarity of the aspect general of location...           1      1  \n1  the polarity of the aspect general of location...           0      0  \n2  the polarity of the aspect general of location...           0      0  \n3  the polarity of the aspect price of location -...           1      1  \n4  the polarity of the aspect price of location -...           0      0  \n5  the polarity of the aspect price of location -...           0      0  \n6  the polarity of the aspect safety of location ...           1      1  \n7  the polarity of the aspect safety of location ...           0      0  \n8  the polarity of the aspect safety of location ...           0      0  \n9  the polarity of the aspect transit location of...           1      1  \n   id                                          sentence1  \\\n0   0  if you want to go out drinking where the cool ...   \n1   0  if you want to go out drinking where the cool ...   \n2   0  if you want to go out drinking where the cool ...   \n3   0  if you want to go out drinking where the cool ...   \n4   0  if you want to go out drinking where the cool ...   \n5   0  if you want to go out drinking where the cool ...   \n6   0  if you want to go out drinking where the cool ...   \n7   0  if you want to go out drinking where the cool ...   \n8   0  if you want to go out drinking where the cool ...   \n9   0  if you want to go out drinking where the cool ...   \n\n                                           sentence2  label_desc  label  \n0  the polarity of the aspect general of location...           1      1  \n1  the polarity of the aspect general of location...           0      0  \n2  the polarity of the aspect general of location...           0      0  \n3  the polarity of the aspect price of location -...           1      1  \n4  the polarity of the aspect price of location -...           0      0  \n5  the polarity of the aspect price of location -...           0      0  \n6  the polarity of the aspect safety of location ...           1      1  \n7  the polarity of the aspect safety of location ...           0      0  \n8  the polarity of the aspect safety of location ...           0      0  \n9  the polarity of the aspect transit location of...           1      1  \n   id                                    sentence1  \\\n0   0  i stayed in location - 1 and loved the area   \n1   0  i stayed in location - 1 and loved the area   \n2   0  i stayed in location - 1 and loved the area   \n3   0  i stayed in location - 1 and loved the area   \n4   0  i stayed in location - 1 and loved the area   \n5   0  i stayed in location - 1 and loved the area   \n6   0  i stayed in location - 1 and loved the area   \n7   0  i stayed in location - 1 and loved the area   \n8   0  i stayed in location - 1 and loved the area   \n9   0  i stayed in location - 1 and loved the area   \n\n                                           sentence2  label_desc  label  \n0  the polarity of the aspect general of location...           0      0  \n1  the polarity of the aspect general of location...           1      1  \n2  the polarity of the aspect general of location...           0      0  \n3  the polarity of the aspect price of location -...           1      1  \n4  the polarity of the aspect price of location -...           0      0  \n5  the polarity of the aspect price of location -...           0      0  \n6  the polarity of the aspect safety of location ...           1      1  \n7  the polarity of the aspect safety of location ...           0      0  \n8  the polarity of the aspect safety of location ...           0      0  \n9  the polarity of the aspect transit location of...           1      1  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# encode sentence ","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\nmodel_ckpt = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_ckpt, do_lower_case=True)\n\ndef tokenizing(df, tokenizer, MAX_LEN = 256):\n\n    sent1 = df.sentence1.values\n    sent2 = df.sentence2.values\n    sentences = [sent1[i] + \"[SEP]\" + sent2[i] for i in range(len(sent1))]\n    labels = df.label.values\n\n    input_ids = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\n    attention_masks = [[float(i>0) for i in input_id]for input_id in input_ids]\n    \n    return input_ids, attention_masks, labels\n\n\ntrain_input_ids, train_attention_masks, train_labels = tokenizing(df_train, tokenizer)\ntest_input_ids, test_attention_masks, test_labels = tokenizing(df_test, tokenizer)\nval_input_ids, val_attention_masks, val_labels = tokenizing(df_val, tokenizer)\n\nprint(f\"train data len: {len(train_input_ids)}\")\nprint(f\"test data len: {len(test_input_ids)}\")\nprint(f\"val data len: {len(val_input_ids)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:14:46.605765Z","iopub.execute_input":"2022-09-19T11:14:46.606442Z","iopub.status.idle":"2022-09-19T11:16:10.056059Z","shell.execute_reply.started":"2022-09-19T11:14:46.606403Z","shell.execute_reply":"2022-09-19T11:16:10.054956Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"train data len: 45024\ntest data len: 22548\nval data len: 11244\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_input_ids[0])\nprint(train_attention_masks[0])\nprint(train_labels[0])\nprint(tokenizer.convert_ids_to_tokens(train_input_ids[0]))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:16:10.057681Z","iopub.execute_input":"2022-09-19T11:16:10.058292Z","iopub.status.idle":"2022-09-19T11:16:10.066280Z","shell.execute_reply.started":"2022-09-19T11:16:10.058248Z","shell.execute_reply":"2022-09-19T11:16:10.064156Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"[101, 3295, 1011, 1016, 1010, 3295, 1011, 1015, 102, 1996, 11508, 3012, 1997, 1996, 7814, 2236, 1997, 3295, 1011, 1015, 1011, 2003, 3904, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n1\n['[CLS]', 'location', '-', '2', ',', 'location', '-', '1', '[SEP]', 'the', 'polar', '##ity', 'of', 'the', 'aspect', 'general', 'of', 'location', '-', '1', '-', 'is', 'none', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# convert all our data into torch tensors, required data type for our model\ntrain_input_ids = torch.tensor(train_input_ids)\ntrain_attention_masks = torch.tensor(train_attention_masks)\ntrain_labels = torch.tensor(train_labels)\n\ntest_input_ids = torch.tensor(test_input_ids)\ntest_attention_masks = torch.tensor(test_attention_masks)\ntest_labels = torch.tensor(test_labels)\n\nval_input_ids = torch.tensor(val_input_ids)\nval_attention_masks = torch.tensor(val_attention_masks)\nval_labels = torch.tensor(val_labels)\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_input_ids,train_attention_masks,train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_input_ids,test_attention_masks,test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_input_ids,val_attention_masks,val_labels)\nval_sampler = RandomSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:16:10.067991Z","iopub.execute_input":"2022-09-19T11:16:10.068833Z","iopub.status.idle":"2022-09-19T11:16:12.140949Z","shell.execute_reply.started":"2022-09-19T11:16:10.068774Z","shell.execute_reply":"2022-09-19T11:16:12.139844Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataloader))\nprint(len(test_dataloader))\nprint(len(val_dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:16:12.142390Z","iopub.execute_input":"2022-09-19T11:16:12.142773Z","iopub.status.idle":"2022-09-19T11:16:12.149253Z","shell.execute_reply.started":"2022-09-19T11:16:12.142737Z","shell.execute_reply":"2022-09-19T11:16:12.148143Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"1407\n705\n352\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# define Model, Hyperparameter, optimizer","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n\n\n\nmodel = BertForSequenceClassification.from_pretrained(model_ckpt, num_labels=len(unique_labels)).to(device)\n\nlr = 2e-5\nadam_epsilon = 1e-8\nepochs = 3\n\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader) * epochs\n\noptimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon, correct_bias = False)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:16:12.150673Z","iopub.execute_input":"2022-09-19T11:16:12.151505Z","iopub.status.idle":"2022-09-19T11:16:18.414595Z","shell.execute_reply.started":"2022-09-19T11:16:12.151469Z","shell.execute_reply":"2022-09-19T11:16:18.413480Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# train model and test","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm, trange,notebook,tqdm_notebook\nimport time\n\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nmodel.zero_grad()\n\n# 用作画图\ntrain_loss_list = []\n\n# 查看训练过程中的learning rate 变化\nlearning_rate = []\n\nfor epoch in notebook.tnrange(1, epochs+1, desc = 'Epoch'):\n    start = time.time()\n    print(\"<\" + \"=\"*22 + f\"Epoch{epoch}, Batch{len(train_dataloader)}\" + \"=\"*22 + \">\")\n    \n    all_loss = 0\n    \n    curSample = 0.0\n    curRight = 0\n    \n    # 开始训练\n    for step, batch in enumerate(train_dataloader):\n        \n        model.train()\n        \n        # 放入gpu中\n        batch = tuple(t.to(device) for t in batch)\n        \n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # tips: BertForSequenceClassifier 输出的第一个是loss,第二个是（batchsize, label_prob）\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels = b_labels)\n        loss = outputs[0]\n        label_prob = outputs[1]\n        \n        label_prob = label_prob.to('cpu').detach().numpy()\n        label_prob = np.argmax(label_prob, axis=1).flatten()\n        b_labels = b_labels.to('cpu').detach().numpy().flatten()\n        curSample += len(b_labels)\n        curRight += (label_prob == b_labels).sum().item()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        scheduler.step()\n        \n        optimizer.zero_grad()\n        \n        all_loss += loss.item()\n        \n        \n        if (step+1) % 50 == 0:\n            print(f\"step: {step+1} loss:{all_loss / (step+1)} time: {time.time() - start} cur acc:{curRight / curSample}\")\n    \n\n    #store the current learning rate\n    for param_group in optimizer.param_groups:\n        print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n        learning_rate.append(param_group['lr'])\n\n        \n    train_loss_list.append(all_loss / len(train_dataloader))\n    print(F'\\n\\tAverage Training loss: {train_loss_list[-1]}')\n    \n    \n    # ================= Validation or Test ================== #\n    def testOrVal(dataloader, mode='test'):\n        model.eval()\n\n        eval_acc, eval_mcc, nb_eval_steps = 0, 0, 0\n\n        for batch in dataloader:\n            batch = tuple(t.to(device) for t in batch)\n\n            b_input_ids, b_input_mask, b_labels = batch\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n            pred = outputs[0].to('cpu').numpy()\n            true_label = b_labels.to('cpu').numpy()\n\n            pred_flat = np.argmax(pred, axis=1).flatten()\n            labels_flat = true_label.flatten()\n\n            tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n            tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n\n            eval_acc += tmp_eval_accuracy\n            eval_mcc += tmp_eval_mcc_accuracy\n            nb_eval_steps += 1\n        if mode == 'val':\n            print(F'\\n\\tValidation Accuracy: {eval_acc/nb_eval_steps}')\n            print(F'\\n\\tValidation MCC Accuracy: {eval_mcc/nb_eval_steps}')\n        else:\n            print(F'\\n\\tTest Accuracy: {eval_acc/nb_eval_steps}')\n            print(F'\\n\\tTest MCC Accuracy: {eval_mcc/nb_eval_steps}')\n    \n    testOrVal(val_dataloader, \"val\")\n    testOrVal(test_dataloader, \"test\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T11:16:18.419334Z","iopub.execute_input":"2022-09-19T11:16:18.420146Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95906448630747cf9e5b6ac629b907f5"}},"metadata":{}},{"name":"stdout","text":"<======================Epoch1, Batch1407======================>\nstep: 50 loss:0.47151624083518984 time: 36.43498754501343 cur acc:0.793125\nstep: 100 loss:0.40114370457828047 time: 72.6937608718872 cur acc:0.8465625\nstep: 150 loss:0.3806666031976541 time: 108.98527002334595 cur acc:0.8620833333333333\nstep: 200 loss:0.36968489430844786 time: 145.27429938316345 cur acc:0.87078125\nstep: 250 loss:0.3638432657122612 time: 181.5859637260437 cur acc:0.874875\nstep: 300 loss:0.35760256052017214 time: 217.87058329582214 cur acc:0.8791666666666667\nstep: 350 loss:0.35557821767670766 time: 254.16728973388672 cur acc:0.8803571428571428\nstep: 400 loss:0.3555995941720903 time: 290.4611418247223 cur acc:0.88046875\nstep: 450 loss:0.35424506838123004 time: 326.72878527641296 cur acc:0.8804166666666666\nstep: 500 loss:0.35132486300170424 time: 363.0337266921997 cur acc:0.88125\nstep: 550 loss:0.3482471444796432 time: 399.3198459148407 cur acc:0.8817613636363636\nstep: 600 loss:0.3451541714494427 time: 435.6393332481384 cur acc:0.8820833333333333\nstep: 650 loss:0.34171520619438245 time: 471.97272992134094 cur acc:0.8830769230769231\nstep: 700 loss:0.3382081605706896 time: 508.3330981731415 cur acc:0.8840178571428572\nstep: 750 loss:0.33499185512463253 time: 544.7217638492584 cur acc:0.8843333333333333\nstep: 800 loss:0.3323104196786881 time: 581.0384523868561 cur acc:0.8847265625\nstep: 850 loss:0.32867713003474125 time: 617.3430233001709 cur acc:0.885625\nstep: 900 loss:0.3254655710193846 time: 653.6960980892181 cur acc:0.8861805555555555\nstep: 950 loss:0.3227229139993065 time: 690.0177989006042 cur acc:0.8866447368421052\nstep: 1000 loss:0.3202547522187233 time: 726.3706741333008 cur acc:0.88696875\nstep: 1050 loss:0.3187550899954069 time: 762.6974012851715 cur acc:0.8870833333333333\nstep: 1100 loss:0.31734701086851685 time: 799.0162220001221 cur acc:0.8872443181818181\nstep: 1150 loss:0.3161554904832788 time: 835.3694243431091 cur acc:0.8871467391304347\nstep: 1200 loss:0.31259392225183547 time: 871.6976163387299 cur acc:0.8875\nstep: 1250 loss:0.3085362720936537 time: 908.0606799125671 cur acc:0.888275\nstep: 1300 loss:0.30467814217106653 time: 944.384085893631 cur acc:0.8888701923076923\nstep: 1350 loss:0.3005447245609981 time: 980.7387363910675 cur acc:0.8895601851851852\nstep: 1400 loss:0.29593138835259847 time: 1017.0281081199646 cur acc:0.8908258928571429\n\n\tCurrent Learning rate:  1.3333333333333333e-05\n\n\tAverage Training loss: 0.29509111289954304\n\n\tValidation Accuracy: 0.9170809659090909\n\n\tValidation MCC Accuracy: 0.8170118891217529\n\n\tTest Accuracy: 0.9148670212765958\n\n\tTest MCC Accuracy: 0.8099617222875014\n<======================Epoch2, Batch1407======================>\nstep: 50 loss:0.171269019395113 time: 36.374674797058105 cur acc:0.929375\nstep: 100 loss:0.14923267032951115 time: 72.72011065483093 cur acc:0.9371875\nstep: 150 loss:0.14711488511413337 time: 109.08534097671509 cur acc:0.9397916666666667\nstep: 200 loss:0.14731974322348834 time: 145.42554640769958 cur acc:0.93953125\nstep: 250 loss:0.14084752011299134 time: 181.74893641471863 cur acc:0.940875\nstep: 300 loss:0.1386573705325524 time: 218.0959153175354 cur acc:0.9420833333333334\nstep: 350 loss:0.1374879277218133 time: 254.498797416687 cur acc:0.943125\nstep: 400 loss:0.1381828709959518 time: 290.9228267669678 cur acc:0.94265625\nstep: 450 loss:0.13664898422339725 time: 327.2735023498535 cur acc:0.9434722222222223\nstep: 500 loss:0.1371130891898647 time: 363.6136882305145 cur acc:0.9436875\nstep: 550 loss:0.13623962480998175 time: 399.99096965789795 cur acc:0.9441477272727272\nstep: 600 loss:0.13453644754442698 time: 436.4719145298004 cur acc:0.9446875\nstep: 650 loss:0.13352101389820187 time: 472.8622796535492 cur acc:0.945\nstep: 700 loss:0.13138785081449897 time: 509.1982979774475 cur acc:0.9460267857142857\nstep: 750 loss:0.13152561837248505 time: 545.581000328064 cur acc:0.9461666666666667\nstep: 800 loss:0.13062701269576793 time: 581.9995365142822 cur acc:0.9465625\nstep: 850 loss:0.12960354425177417 time: 618.3567681312561 cur acc:0.9470955882352942\nstep: 900 loss:0.12940443379649272 time: 654.7214305400848 cur acc:0.9471875\nstep: 950 loss:0.12795283966011514 time: 691.100861787796 cur acc:0.9478618421052631\nstep: 1000 loss:0.12732016668422147 time: 727.4676485061646 cur acc:0.94809375\nstep: 1050 loss:0.12623212122979263 time: 763.84911942482 cur acc:0.9486309523809524\nstep: 1100 loss:0.12520213737914507 time: 800.2515952587128 cur acc:0.9490340909090909\nstep: 1150 loss:0.12350858469534179 time: 836.6583216190338 cur acc:0.9497554347826087\nstep: 1200 loss:0.12295161151792854 time: 873.0332651138306 cur acc:0.9498697916666666\nstep: 1250 loss:0.12200604178234935 time: 909.41117811203 cur acc:0.950275\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}