{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-19T08:52:14.539875Z","iopub.execute_input":"2022-09-19T08:52:14.540261Z","iopub.status.idle":"2022-09-19T08:52:14.555079Z","shell.execute_reply.started":"2022-09-19T08:52:14.540209Z","shell.execute_reply":"2022-09-19T08:52:14.553956Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"/kaggle/input/bert-nli-m-for-absa/dev_NLI_M.tsv\n/kaggle/input/bert-nli-m-for-absa/test_NLI_M.tsv\n/kaggle/input/bert-nli-m-for-absa/train_NLI_M.tsv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:14.557944Z","iopub.execute_input":"2022-09-19T08:52:14.558512Z","iopub.status.idle":"2022-09-19T08:52:23.990192Z","shell.execute_reply.started":"2022-09-19T08:52:14.558476Z","shell.execute_reply":"2022-09-19T08:52:23.989005Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\n# identify and specify the GPU as the device, later in training loop we will load data into device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)\n\nSEED = 19\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif device == torch.device(\"cuda\"):\n    torch.cuda.manual_seed_all(SEED)\n\n    \nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:23.992073Z","iopub.execute_input":"2022-09-19T08:52:23.992498Z","iopub.status.idle":"2022-09-19T08:52:24.001527Z","shell.execute_reply.started":"2022-09-19T08:52:23.992456Z","shell.execute_reply":"2022-09-19T08:52:24.000453Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# loading and preprocessing data","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf_train = pd.read_csv(\"/kaggle/input/bert-nli-m-for-absa/train_NLI_M.tsv\",skiprows=1, delimiter='\\t', header = None, names=['id','sentence1','sentence2','label'])\ndf_test = pd.read_csv(\"/kaggle/input/bert-nli-m-for-absa/test_NLI_M.tsv\",skiprows=1, delimiter='\\t', header = None,names=['id','sentence1','sentence2','label'])\ndf_val = pd.read_csv(\"/kaggle/input/bert-nli-m-for-absa/dev_NLI_M.tsv\",skiprows=1, delimiter='\\t', header = None,names=['id','sentence1','sentence2','label'])\n\nprint(f\"train data len: {len(df_train)}\")\nprint(f\"test data len: {len(df_test)}\")\nprint(f\"val data len: {len(df_val)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:24.004511Z","iopub.execute_input":"2022-09-19T08:52:24.005620Z","iopub.status.idle":"2022-09-19T08:52:24.058993Z","shell.execute_reply.started":"2022-09-19T08:52:24.005583Z","shell.execute_reply":"2022-09-19T08:52:24.057941Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"train data len: 15008\ntest data len: 7516\nval data len: 3748\n","output_type":"stream"}]},{"cell_type":"code","source":"print(df_train[:10])\n\nprint(\"\\n\")\nprint(df_train['label'].unique())\n\nunique_labels = df_train['label'].unique()","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:24.060312Z","iopub.execute_input":"2022-09-19T08:52:24.060646Z","iopub.status.idle":"2022-09-19T08:52:24.075038Z","shell.execute_reply.started":"2022-09-19T08:52:24.060611Z","shell.execute_reply":"2022-09-19T08:52:24.073898Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"     id                                          sentence1  \\\n0     0                        location - 2 , location - 1   \n1     0                        location - 2 , location - 1   \n2     0                        location - 2 , location - 1   \n3     0                        location - 2 , location - 1   \n4  1000  location - 1 is one of the most expensive area...   \n5  1000  location - 1 is one of the most expensive area...   \n6  1000  location - 1 is one of the most expensive area...   \n7  1000  location - 1 is one of the most expensive area...   \n8  1001  the hard rock cafe is close by , just by locat...   \n9  1001  the hard rock cafe is close by , just by locat...   \n\n                         sentence2     label  \n0           location - 1 - general      None  \n1             location - 1 - price      None  \n2            location - 1 - safety      None  \n3  location - 1 - transit location      None  \n4           location - 1 - general      None  \n5             location - 1 - price  Negative  \n6            location - 1 - safety      None  \n7  location - 1 - transit location      None  \n8           location - 1 - general      None  \n9             location - 1 - price      None  \n\n\n['None' 'Negative' 'Positive']\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# 将字符串映射成整型\ndef train_str_2_int(df):\n    labelEncoder = LabelEncoder()\n    df['label_enc'] = labelEncoder.fit_transform(df['label'])\n    \n    # 原地给列改名\n    df.rename(columns = {'label': 'label_desc'}, inplace = True)\n    df.rename(columns = {'label_enc': 'label'}, inplace = True)\n\n    \ntrain_str_2_int(df_train)\ntrain_str_2_int(df_test)\ntrain_str_2_int(df_val)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:24.076691Z","iopub.execute_input":"2022-09-19T08:52:24.077668Z","iopub.status.idle":"2022-09-19T08:52:24.093905Z","shell.execute_reply.started":"2022-09-19T08:52:24.077633Z","shell.execute_reply":"2022-09-19T08:52:24.092907Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"print(df_train[0:10])\nprint(df_test[0:10])\nprint(df_val[0:10])","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:24.095203Z","iopub.execute_input":"2022-09-19T08:52:24.095726Z","iopub.status.idle":"2022-09-19T08:52:24.110956Z","shell.execute_reply.started":"2022-09-19T08:52:24.095690Z","shell.execute_reply":"2022-09-19T08:52:24.109751Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"     id                                          sentence1  \\\n0     0                        location - 2 , location - 1   \n1     0                        location - 2 , location - 1   \n2     0                        location - 2 , location - 1   \n3     0                        location - 2 , location - 1   \n4  1000  location - 1 is one of the most expensive area...   \n5  1000  location - 1 is one of the most expensive area...   \n6  1000  location - 1 is one of the most expensive area...   \n7  1000  location - 1 is one of the most expensive area...   \n8  1001  the hard rock cafe is close by , just by locat...   \n9  1001  the hard rock cafe is close by , just by locat...   \n\n                         sentence2 label_desc  label  \n0           location - 1 - general       None      1  \n1             location - 1 - price       None      1  \n2            location - 1 - safety       None      1  \n3  location - 1 - transit location       None      1  \n4           location - 1 - general       None      1  \n5             location - 1 - price   Negative      0  \n6            location - 1 - safety       None      1  \n7  location - 1 - transit location       None      1  \n8           location - 1 - general       None      1  \n9             location - 1 - price       None      1  \n     id                                          sentence1  \\\n0     0  if you want to go out drinking where the cool ...   \n1     0  if you want to go out drinking where the cool ...   \n2     0  if you want to go out drinking where the cool ...   \n3     0  if you want to go out drinking where the cool ...   \n4  1000  location - 1 actually has quite a cool feel to...   \n5  1000  location - 1 actually has quite a cool feel to...   \n6  1000  location - 1 actually has quite a cool feel to...   \n7  1000  location - 1 actually has quite a cool feel to...   \n8  1001  there ' s location - 1 , which is , i would sa...   \n9  1001  there ' s location - 1 , which is , i would sa...   \n\n                         sentence2 label_desc  label  \n0           location - 1 - general       None      1  \n1             location - 1 - price       None      1  \n2            location - 1 - safety       None      1  \n3  location - 1 - transit location       None      1  \n4           location - 1 - general   Positive      2  \n5             location - 1 - price       None      1  \n6            location - 1 - safety       None      1  \n7  location - 1 - transit location       None      1  \n8           location - 1 - general       None      1  \n9             location - 1 - price       None      1  \n    id                                          sentence1  \\\n0    0        i stayed in location - 1 and loved the area   \n1    0        i stayed in location - 1 and loved the area   \n2    0        i stayed in location - 1 and loved the area   \n3    0        i stayed in location - 1 and loved the area   \n4  100  personally , i do n't think location - 1 is th...   \n5  100  personally , i do n't think location - 1 is th...   \n6  100  personally , i do n't think location - 1 is th...   \n7  100  personally , i do n't think location - 1 is th...   \n8  101  in london outskirts ( south of london ) there ...   \n9  101  in london outskirts ( south of london ) there ...   \n\n                         sentence2 label_desc  label  \n0           location - 1 - general   Positive      2  \n1             location - 1 - price       None      1  \n2            location - 1 - safety       None      1  \n3  location - 1 - transit location       None      1  \n4           location - 1 - general   Positive      2  \n5             location - 1 - price       None      1  \n6            location - 1 - safety       None      1  \n7  location - 1 - transit location       None      1  \n8           location - 1 - general       None      1  \n9             location - 1 - price   Positive      2  \n","output_type":"stream"}]},{"cell_type":"markdown","source":"# encode sentence ","metadata":{}},{"cell_type":"code","source":"from transformers import BertTokenizer\n\nmodel_ckpt = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_ckpt, do_lower_case=True)\n\ndef tokenizing(df, tokenizer, MAX_LEN = 256):\n\n    sent1 = df.sentence1.values\n    sent2 = df.sentence2.values\n    sentences = [sent1[i] + \"[SEP]\" + sent2[i] for i in range(len(sent1))]\n    labels = df.label.values\n\n    input_ids = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN,pad_to_max_length=True) for sent in sentences]\n    attention_masks = [[float(i>0) for i in input_id]for input_id in input_ids]\n    \n    return input_ids, attention_masks, labels\n\n\ntrain_input_ids, train_attention_masks, train_labels = tokenizing(df_train, tokenizer)\ntest_input_ids, test_attention_masks, test_labels = tokenizing(df_test, tokenizer)\nval_input_ids, val_attention_masks, val_labels = tokenizing(df_val, tokenizer)\n\nprint(f\"train data len: {len(train_input_ids)}\")\nprint(f\"test data len: {len(test_input_ids)}\")\nprint(f\"val data len: {len(val_input_ids)}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:24.112715Z","iopub.execute_input":"2022-09-19T08:52:24.113087Z","iopub.status.idle":"2022-09-19T08:52:48.132372Z","shell.execute_reply.started":"2022-09-19T08:52:24.113052Z","shell.execute_reply":"2022-09-19T08:52:48.131214Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","output_type":"stream"},{"name":"stdout","text":"train data len: 15008\ntest data len: 7516\nval data len: 3748\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train_input_ids[0])\nprint(train_attention_masks[0])\nprint(train_labels[0])\nprint(tokenizer.convert_ids_to_tokens(train_input_ids[0]))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:48.134195Z","iopub.execute_input":"2022-09-19T08:52:48.134606Z","iopub.status.idle":"2022-09-19T08:52:48.140800Z","shell.execute_reply.started":"2022-09-19T08:52:48.134567Z","shell.execute_reply":"2022-09-19T08:52:48.139663Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[101, 3295, 1011, 1016, 1010, 3295, 1011, 1015, 102, 3295, 1011, 1015, 1011, 2236, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n1\n['[CLS]', 'location', '-', '2', ',', 'location', '-', '1', '[SEP]', 'location', '-', '1', '-', 'general', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# convert all our data into torch tensors, required data type for our model\ntrain_input_ids = torch.tensor(train_input_ids)\ntrain_attention_masks = torch.tensor(train_attention_masks)\ntrain_labels = torch.tensor(train_labels)\n\ntest_input_ids = torch.tensor(test_input_ids)\ntest_attention_masks = torch.tensor(test_attention_masks)\ntest_labels = torch.tensor(test_labels)\n\nval_input_ids = torch.tensor(val_input_ids)\nval_attention_masks = torch.tensor(val_attention_masks)\nval_labels = torch.tensor(val_labels)\n\n# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\ntrain_data = TensorDataset(train_input_ids,train_attention_masks,train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\ntest_data = TensorDataset(test_input_ids,test_attention_masks,test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\nval_data = TensorDataset(val_input_ids,val_attention_masks,val_labels)\nval_sampler = RandomSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:48.144286Z","iopub.execute_input":"2022-09-19T08:52:48.144767Z","iopub.status.idle":"2022-09-19T08:52:48.930681Z","shell.execute_reply.started":"2022-09-19T08:52:48.144732Z","shell.execute_reply":"2022-09-19T08:52:48.929580Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"print(len(train_dataloader))\nprint(len(test_dataloader))\nprint(len(val_dataloader))","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:48.931912Z","iopub.execute_input":"2022-09-19T08:52:48.932328Z","iopub.status.idle":"2022-09-19T08:52:48.938946Z","shell.execute_reply.started":"2022-09-19T08:52:48.932289Z","shell.execute_reply":"2022-09-19T08:52:48.937702Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"469\n235\n118\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# define Model, Hyperparameter, optimizer","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig,AdamW, BertForSequenceClassification,get_linear_schedule_with_warmup\n\n\n\nmodel = BertForSequenceClassification.from_pretrained(model_ckpt, num_labels=len(unique_labels)).to(device)\n\nlr = 2e-5\nadam_epsilon = 1e-8\nepochs = 3\n\nnum_warmup_steps = 0\nnum_training_steps = len(train_dataloader) * epochs\n\noptimizer = AdamW(model.parameters(), lr = lr, eps = adam_epsilon, correct_bias = False)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:48.940611Z","iopub.execute_input":"2022-09-19T08:52:48.941251Z","iopub.status.idle":"2022-09-19T08:52:51.739689Z","shell.execute_reply.started":"2022-09-19T08:52:48.941174Z","shell.execute_reply":"2022-09-19T08:52:51.737645Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  FutureWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# train model and test","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm, trange,notebook,tqdm_notebook\nimport time\n\nfrom sklearn.metrics import confusion_matrix,classification_report\n# Import and evaluate each test batch using Matthew's correlation coefficient\nfrom sklearn.metrics import accuracy_score,matthews_corrcoef\n\nmodel.zero_grad()\n\n# 用作画图\ntrain_loss_list = []\n\n# 查看训练过程中的learning rate 变化\nlearning_rate = []\n\nfor epoch in notebook.tnrange(1, epochs+1, desc = 'Epoch'):\n    start = time.time()\n    print(\"<\" + \"=\"*22 + f\"Epoch{epoch}, Batch{len(train_dataloader)}\" + \"=\"*22 + \">\")\n    \n    all_loss = 0\n    \n    curSample = 0.0\n    curRight = 0\n    \n    # 开始训练\n    for step, batch in enumerate(train_dataloader):\n        \n        model.train()\n        \n        # 放入gpu中\n        batch = tuple(t.to(device) for t in batch)\n        \n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # tips: BertForSequenceClassifier 输出的第一个是loss,第二个是（batchsize, label_prob）\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels = b_labels)\n        loss = outputs[0]\n        label_prob = outputs[1]\n        \n        label_prob = label_prob.to('cpu').detach().numpy()\n        label_prob = np.argmax(label_prob, axis=1).flatten()\n        b_labels = b_labels.to('cpu').detach().numpy().flatten()\n        curSample += len(b_labels)\n        curRight += (label_prob == b_labels).sum().item()\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        scheduler.step()\n        \n        optimizer.zero_grad()\n        \n        all_loss += loss.item()\n        \n        \n        if (step+1) % 50 == 0:\n            print(f\"step: {step+1} loss:{all_loss / (step+1)} time: {time.time() - start} cur acc:{curRight / curSample}\")\n    \n\n    #store the current learning rate\n    for param_group in optimizer.param_groups:\n        print(\"\\n\\tCurrent Learning rate: \",param_group['lr'])\n        learning_rate.append(param_group['lr'])\n\n        \n    train_loss_list.append(all_loss / len(train_dataloader))\n    print(F'\\n\\tAverage Training loss: {train_loss_list[-1]}')\n    \n    \n    # ================= Validation or Test ================== #\n    def testOrVal(dataloader, mode='test'):\n        model.eval()\n\n        eval_acc, eval_mcc, nb_eval_steps = 0, 0, 0\n\n        for batch in dataloader:\n            batch = tuple(t.to(device) for t in batch)\n\n            b_input_ids, b_input_mask, b_labels = batch\n\n            with torch.no_grad():\n                outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n            pred = outputs[0].to('cpu').numpy()\n            true_label = b_labels.to('cpu').numpy()\n\n            pred_flat = np.argmax(pred, axis=1).flatten()\n            labels_flat = true_label.flatten()\n\n            tmp_eval_accuracy = accuracy_score(labels_flat,pred_flat)\n            tmp_eval_mcc_accuracy = matthews_corrcoef(labels_flat, pred_flat)\n\n            eval_acc += tmp_eval_accuracy\n            eval_mcc += tmp_eval_mcc_accuracy\n            nb_eval_steps += 1\n        if mode == 'val':\n            print(F'\\n\\tValidation Accuracy: {eval_acc/nb_eval_steps}')\n            print(F'\\n\\tValidation MCC Accuracy: {eval_mcc/nb_eval_steps}')\n        else:\n            print(F'\\n\\tTest Accuracy: {eval_acc/nb_eval_steps}')\n            print(F'\\n\\tTest MCC Accuracy: {eval_mcc/nb_eval_steps}')\n    \n    testOrVal(val_dataloader, \"val\")\n    testOrVal(test_dataloader, \"test\")","metadata":{"execution":{"iopub.status.busy":"2022-09-19T08:52:51.741408Z","iopub.execute_input":"2022-09-19T08:52:51.741962Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c01aa6b418f48e5badd699df7d4bae7"}},"metadata":{}},{"name":"stdout","text":"<======================Epoch1, Batch469======================>\nstep: 50 loss:0.5634330296516419 time: 36.349562644958496 cur acc:0.825\nstep: 100 loss:0.5383626091480255 time: 72.6694655418396 cur acc:0.8334375\nstep: 150 loss:0.5051880579193433 time: 108.96956896781921 cur acc:0.8358333333333333\nstep: 200 loss:0.47928505077958106 time: 145.29730319976807 cur acc:0.84\nstep: 250 loss:0.4481838331222534 time: 181.6266827583313 cur acc:0.846125\nstep: 300 loss:0.42155152489741643 time: 217.93274569511414 cur acc:0.8495833333333334\nstep: 350 loss:0.40128058261104993 time: 254.26694321632385 cur acc:0.8551785714285715\nstep: 400 loss:0.38559797398746015 time: 290.61002373695374 cur acc:0.86\nstep: 450 loss:0.36842714801430704 time: 326.8997440338135 cur acc:0.8659722222222223\n\n\tCurrent Learning rate:  1.3333333333333333e-05\n\n\tAverage Training loss: 0.36464517150542886\n\n\tValidation Accuracy: 0.9221398305084746\n\n\tValidation MCC Accuracy: 0.6951273102349445\n\n\tTest Accuracy: 0.9199088145896656\n\n\tTest MCC Accuracy: 0.6911501456835656\n<======================Epoch2, Batch469======================>\nstep: 50 loss:0.1561599523574114 time: 36.336567640304565 cur acc:0.94375\nstep: 100 loss:0.1616286606155336 time: 72.66413760185242 cur acc:0.9425\nstep: 150 loss:0.1671756205956141 time: 108.98190236091614 cur acc:0.940625\nstep: 200 loss:0.16934806732460855 time: 145.3143973350525 cur acc:0.93734375\nstep: 250 loss:0.16538635464012622 time: 181.62833857536316 cur acc:0.9395\nstep: 300 loss:0.1652039324492216 time: 217.99168920516968 cur acc:0.9394791666666666\nstep: 350 loss:0.1641198473370501 time: 254.42658805847168 cur acc:0.9403571428571429\nstep: 400 loss:0.1620794632891193 time: 290.79256796836853 cur acc:0.940859375\nstep: 450 loss:0.15977506700075336 time: 327.1628816127777 cur acc:0.9411805555555556\n\n\tCurrent Learning rate:  6.666666666666667e-06\n\n\tAverage Training loss: 0.15970366307174855\n\n\tValidation Accuracy: 0.9314088983050848\n\n\tValidation MCC Accuracy: 0.749789367516218\n\n\tTest Accuracy: 0.9237272036474165\n\n\tTest MCC Accuracy: 0.7243800940690271\n<======================Epoch3, Batch469======================>\nstep: 50 loss:0.10310484103858471 time: 36.37421703338623 cur acc:0.965625\nstep: 100 loss:0.10535093262791634 time: 72.73977613449097 cur acc:0.96\nstep: 150 loss:0.10144103077550728 time: 109.08917665481567 cur acc:0.9610416666666667\nstep: 200 loss:0.10022854697424918 time: 145.46991682052612 cur acc:0.9625\nstep: 250 loss:0.10089404764957725 time: 181.85650038719177 cur acc:0.962875\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}